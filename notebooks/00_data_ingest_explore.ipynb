{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC 311 Complaints 2017\n",
    "\n",
    "For more information on this dataset, see the following articles:\n",
    "* The _Wired_ magazine article [\"What a Hundred Million Calls to 311 Reveal About New York.\"](https://www.wired.com/2010/11/ff_311_new_york/).\n",
    "* Ariel White and Kris-Stella Trump's research paper [\"The Promises and Pitfalls of 311 Data\"](https://arwhite.mit.edu/sites/default/files/images/White%20Trump%20-%20Promises%20Pitfalls%20311%20Data%20-%20UAR%202017.pdf) in [_Urban Affairs Review_](https://journals.sagepub.com/doi/abs/10.1177/1078087416673202).\n",
    "\n",
    "I used the White and Trump paper, in paritcular, when the making data-cleaning assumptions (e.g., what to clean vs. what to remove) for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import HTTPError\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sodapy import Socrata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sourcing\n",
    "\n",
    "The following data sources were used:\n",
    "\n",
    "* [NYC 311 Complaints](https://nycopendata.socrata.com/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9)\n",
    "* [2010 Census Population by ZIP Code](https://blog.splitwise.com/2013/09/18/the-2010-us-census-population-by-zip-code-totally-free/)\n",
    "* [NYC Neighborhod ZIP Codes](https://www.health.ny.gov/statistics/cancer/registry/appendix/neighborhoods.htm)\n",
    "\n",
    "Inspection of the data revealed non-NYC cities and ZIP codes. Thus, as the focus of this project is on NYC-related complaints, complaints associated with non-NYC locations are removed from the dataset.\n",
    "\n",
    "### 311 Complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow\n",
    "complaints = pd.read_parquet('data/nyc-311-complaints.parquet.gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Progamatically Sourcing the Data\n",
    "\n",
    "The [`sodapy`](https://github.com/xmunoz/sodapy) package used to interface with the [Socrata Open Data (SODA) API](https://dev.socrata.com/) and return the raw 311 complaints data in JSON format. Let's define two (perhaps unnecessary) utility functions, one to format the fields to use in the SoQL query, and another to safely pull the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_fields(*fields):\n",
    "    \"\"\" Formats arbitrary nubmer of fields for the SoQL query \"\"\"\n",
    "    return ','.join(fields)\n",
    "\n",
    "\n",
    "def get_complaints_data(*fields, year=2017, limit=20000000):\n",
    "    \"\"\" Pull certain `fields` for a given `year` from NYC Open Data's\n",
    "    SODA API \"\"\"\n",
    "    try:\n",
    "        with Socrata('data.cityofnewyork.us', None) as client:\n",
    "            results = client.get(\n",
    "                'fhrw-4uyv',\n",
    "                content_type='json',\n",
    "                select=format_fields(*fields),\n",
    "                where=f\"created_date between '{year}-01-01T00:00:00' and '{year}-12-31T23:59:59'\",\n",
    "                limit=limit\n",
    "            )\n",
    "        return results\n",
    "    except HTTPError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use these functions to get our hands on the 311 data for 2017! Note that we're not making a request with an app token, so we'll get a warning that we'll safely ignore for the purposes of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = get_complaints_data(\n",
    "    'unique_key',\n",
    "    'created_date',\n",
    "    'complaint_type',\n",
    "    'descriptor',\n",
    "    'borough',\n",
    "    'city',\n",
    "    'incident_zip',\n",
    "    'incident_address',\n",
    "    'latitude',\n",
    "    'longitude'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints = pd.DataFrame.from_records(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to datetime takes awhile, so a TODO is to find a faster way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that city is _often_ neighborhood, but borough is also logged here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints = complaints.replace('Unspecified', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints[complaints['borough'].isnull()].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just add a small sanity check to make sure we only have data from 2017..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert complaints.created_date.min().year == 2017\n",
    "assert complaints.created_date.max().year == 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating Missing Values\n",
    "\n",
    "First, let's look at the percentage of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints.apply(lambda x: round(100 * x.isnull().mean(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints.borough.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints.query(\"borough == 'Unspecified'\").isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have unspecified boroughs and our analysis is borough-based, for the present analysis we can simply replace the non-borough cities with missing values and fill in any null borough values with any bourgh values logged in the `city` field, and then remove the now redundant `city` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints['city'].isin(['Bronx', 'Brooklyn', 'Manhattan', 'Queens', 'Staten Island'])\n",
    "complaints.loc[is_borough, 'city'] = np.nan\n",
    "complaints.loc[:, 'borough'] = (\n",
    "    complaints.loc[:, 'borough']\n",
    "              .combine_first(complaints['city'])\n",
    ")\n",
    "complaints.drop('city', axis=1, inplace=True)\n",
    "complaints.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population Data by ZIP Code\n",
    "\n",
    "Note that this dataset technically contains [ZCTA (ZIP Code Tabulation Area)](https://www.census.gov/geo/reference/zctas.html) codes, not ZIP codes. However, they are treated as equivalent for the purpose of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_population_by_zip(url):\n",
    "    \"\"\" Gets 2010 Census population by ZIP code data \"\"\"\n",
    "    try:\n",
    "        population_by_zip = pd.read_csv(url)\n",
    "        return population_by_zip\n",
    "    except HTTPError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_url = ('https://s3.amazonaws.com/SplitwiseBlogJB/'\n",
    "                         '2010+Census+Population+By+Zipcode+(ZCTA).csv')\n",
    "\n",
    "population_by_zip = get_population_by_zip(population_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_by_zip.columns = 'zip_code population'.split(' ')\n",
    "population_by_zip.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYC ZIP Code Data\n",
    "\n",
    "As mentioned above, brief inspection of the 311 complaints dataset revealed cities that are not in NYC. Thus, as part of my preprocessing I filtered out these observations.\n",
    "\n",
    "Data on the ZIP codes associated with NYC neighborhoods comes from the New York State Department of Health. The HTML table this data is stored in is not in the most convenient format for data analysis, so the data must be scraped and turned into [\"tidy\"](https://r4ds.had.co.nz/tidy-data.html) format (i.e., basically a statistician's analogue of Codd's [3NF](https://en.wikipedia.org/wiki/Third_normal_form) familiar to data engineers).\n",
    "\n",
    "First, let's create two utility functions, the first to scrape the table from the Department of Health website, and the second to \"tidy\" the data into a minimal table of borough by ZIP code data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_nyc_zips(url):\n",
    "    \"\"\" Scrapes table of NYC zipcodes from NYC Department of Health \"\"\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        return r\n",
    "    except HTTPError as e:\n",
    "        print(\"NYC neighborhood ZIP code lookup table not found:\", e)\n",
    "\n",
    "\n",
    "# TODO: Refactor to have utility functions for, for example, the\n",
    "# \"tidying\" aspects and the conversion aspects ... and rename this\n",
    "# function to something more sensible\n",
    "def tidy_nyc_zips(html):\n",
    "    \"\"\" Wrangle HTML table of NYC ZIP codes into a \"tidy\" data frame\n",
    "\n",
    "    Args:\n",
    "        html (requests.models.Response):\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: This seems too ugly and hacky so find a more elegant\n",
    "    # solution\n",
    "    borough_zips = (\n",
    "        pd.read_html(html.content, header=0)[0]\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    borough_zips.loc[borough_zips['ZIP Codes'].isnull(), 'Borough'] = np.nan\n",
    "    borough_zips.loc[:, 'ZIP Codes'] = \\\n",
    "        borough_zips.loc[:, 'ZIP Codes'].str.replace(' ', '')\n",
    "\n",
    "    borough_zips.loc[:, 'ZIP Codes'] = (\n",
    "        borough_zips.loc[:, 'ZIP Codes']\n",
    "                    .combine_first(borough_zips['Neighborhood'])\n",
    "    )\n",
    "\n",
    "    # TODO: keep the neighborhood information, even though it's not\n",
    "    # currently necessary for this analysis\n",
    "    borough_zips.drop('Neighborhood', axis=1, inplace=True)\n",
    "    borough_zips.loc[:, 'Borough'] = \\\n",
    "        borough_zips.loc[:, 'Borough'].ffill()\n",
    "\n",
    "    # Overwrite the comma-separated string \"list\" in the cell\n",
    "    # with an actual list of integers\n",
    "    borough_zips.loc[:, 'ZIP Codes'] = (\n",
    "        borough_zips.loc[:, 'ZIP Codes']\n",
    "                    .apply(lambda x: x.split(','))\n",
    "    )\n",
    "\n",
    "    # TODO: Write utility function for this pattern\n",
    "    borough_zips = (\n",
    "        borough_zips.set_index(['index', 'Borough'])\n",
    "                    .loc[:, 'ZIP Codes']\n",
    "                    .apply(pd.Series) # Expand the list of \n",
    "                    .stack()\n",
    "                    .reset_index()\n",
    "    )\n",
    "\n",
    "    borough_zips.drop(['index', 'level_2'], axis=1, inplace=True)\n",
    "    borough_zips.columns = \\\n",
    "        'borough zip_code'.split(' ')\n",
    "    borough_zips.loc[:, 'zip_code'] = \\\n",
    "        borough_zips.loc[:, 'zip_code'].astype(int)\n",
    "\n",
    "    return borough_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_zips_url = ('https://www.health.ny.gov/statistics/cancer/registry/appendix/'\n",
    "                'neighborhoods.htm')\n",
    "\n",
    "html = scrape_nyc_zips(nyc_zips_url)\n",
    "nyc_zips = tidy_nyc_zips(html)\n",
    "\n",
    "nyc_zips.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_by_zip_nyc = nyc_zips.merge(\n",
    "    population_by_zip,\n",
    "    on='zip_code',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_by_zip_nyc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_by_zip_nyc[population_by_zip_nyc['population'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ZIP code 11695 is (according to Google) Far Rockaway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Given the size of the dataset (viz., about 19.5 million observations), string values are converted to `pandas` [categorical](https://pandas.pydata.org/pandas-docs/stable/categorical.html) variables, which are internally stored as integers and thus cut down on memory usage when slicing and dicing the data frame.\n",
    "\n",
    "### Data Type Conversion\n",
    "\n",
    "The main idea here is to cut down on the number of variables stored as text in order to decrease the memory used by `pandas`, which at the start is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints.loc[:, 'unique_key'] = \\\n",
    "    complaints.loc[:, 'unique_key'].astype(int)\n",
    "\n",
    "complaints.loc[complaints['borough'].eq('Unspecified'), 'borough'] = None\n",
    "\n",
    "complaints.loc[:, 'borough'] = \\\n",
    "    complaints.loc[:, 'borough'].astype('category')\n",
    "\n",
    "complaints.loc[:, 'complaint_type'] = \\\n",
    "    complaints.loc[:, 'complaint_type'].astype('category')\n",
    "\n",
    "complaints.loc[:, 'created_date'] = \\\n",
    "    complaints.loc[:, 'created_date'].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints.memory_usage()\n",
    "complaints.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints.loc[:, 'incident_zip'] = \\\n",
    "    complaints.loc[:, 'incident_zip'].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Values for \"Unspecified\" Boroughs\n",
    "\n",
    "Brief exploration of the 311 complaints dataset revels that borough is missing for many incidents associated with Queens. For example, a value of `Unspecified` shows up for Forest Hills, Hollis, and other neighborhoods in Queens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_unspecified = complaints['borough'].eq('Unspecified')\n",
    "complaints.loc[is_unspecified, 'borough'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints[complaints['borough'].isnull() & complaints['city'].isnull()].sample(10)\n",
    "complaints[complaints['complaint_type'].str.contains('request', case=False)].complaint_type.value_counts()\n",
    "is_request = complaints['complaint_type'].str.contains('request', case=False)\n",
    "is_benefit = complaints['complaint_type'].eq('Benefit Card Replacement')\n",
    "no_requests = complaints[~is_request & ~is_benefit].copy()\n",
    "# Benefit Card Replacement\n",
    "# DCA / DOH New License Application Request\n",
    "# DOF Parking - Payment Issue\n",
    "# School Maintenance\n",
    "# Literature Request\n",
    "# Street Light Condition\n",
    "# Forms\n",
    "# Illegal Parking\n",
    "# DOF Parking - DMV Clearance\n",
    "# DCA / DOH New License Application Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_requests[no_requests['borough'].isnull() & complaints['city'].isnull()].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints['city'].value_counts()\n",
    "complaints[complaints['borough'].isnull() & complaints['city'].isnull()].park_borough.value_counts()\n",
    "complaints[complaints['complaint_type'].str.contains('request', case=False)].complaint_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Minimal) Text Standardization and Cleaning\n",
    "\n",
    "`ALL CAPS` values were changed to `Title Case` for the `borough` and `city` fields.\n",
    "\n",
    "No further cleanup or standardization was attempted `complaint_type` or `descriptor` fields, as the capitalizations and conventions here (e.g., acronmyms) here often appear meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints.loc[:, 'borough'] = complaints.loc[:, 'borough'].str.title()\n",
    "complaints.loc[:, 'city'] = complaints.loc[:, 'city'].str.title()\n",
    "complaints.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the last of the \"Unspecified\" boroughs that we can (given the minimal cleanup and data sourcing that we've done)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow_start = complaints.shape[0]\n",
    "complaints = complaints.merge(\n",
    "    population_by_zip_nyc,\n",
    "    left_on='incident_zip',\n",
    "    right_on='zip_code',\n",
    "    how='left'\n",
    ")\n",
    "assert complaints.shape[0] == nrow_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints.loc[:, 'borough'] = (\n",
    "    complaints.loc[:, 'borough_x']\n",
    "              .combine_first(complaints['borough_y'])\n",
    ")\n",
    "complaints.drop(['borough_x', 'borough_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_borough = complaints['city'].isin(['Brooklyn', 'Queens', 'Staten Island', 'Manhattan', 'Bronx'])\n",
    "complaints.loc[~is_borough, 'complaints'] = np.nan\n",
    "\n",
    "complaints.loc[:, 'borough'] = (\n",
    "    complaints.loc[:, 'borough']\n",
    "              .combine_first(complaints['city'])\n",
    ")\n",
    "\n",
    "complaints.loc[:, 'borough'] = (\n",
    "    complaints.loc[:, 'borough_x']\n",
    "              .combine_first(complaints['borough_y'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Non-NYC Observations\n",
    "\n",
    "Given that the questions in this exercise deal with NYC, any complaint associated with a ZIP code outside of the city's boundaries is removed.\n",
    "\n",
    "<img src=\"geocode.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_nyc = complaints['zipcode'].isin(nyc_zips['zipcode'])\n",
    "complaints = complaints.loc[is_nyc, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's much more I'd like to do to clean up the dataset and impute missing values ... but you've got to stop somewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping Up\n",
    "\n",
    "The cleaned version of the dataset is then saved for later analysis using the columnar [Apache Parquet](https://en.wikipedia.org/wiki/Apache_Parquet) format, which is significantly faster to read than the corresponding CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints.to_parquet('data/nyc-311-complaints.parquet.gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
