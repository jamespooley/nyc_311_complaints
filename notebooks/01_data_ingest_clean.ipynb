{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join as opj\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import HTTPError, ConnectionError\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sodapy import Socrata\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't currently know why I have to do the following to get `lxml` to work inside the container, but a variety of changes to the `jupyter/Dockerfile` specification did not work and the first five StackOverflow suggestions didn't obviate the need for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a total hack, I need (for my own sake) to find out what's preventing this, but it works for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POPULATION_DATASET_URL = ('https://s3.amazonaws.com/SplitwiseBlogJB/'\n",
    "                          '2010+Census+Population+By+Zipcode+(ZCTA).csv')\n",
    "\n",
    "NYC_ZIPCODES_DATASET_URL = ('https://www.health.ny.gov/statistics/cancer/'\n",
    "                            'registry/appendix/neighborhoods.htm')\n",
    "\n",
    "COMPLAINTS_DATASET_URL = 'data.cityofnewyork.us'\n",
    "DATASET_ID = 'fhrw-4uyv' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 311 Data Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_records(year=2017):\n",
    "    query = \"\"\"\n",
    "    select count(unique_key)\n",
    "    where date_extract_y(created_date) = 2017\n",
    "    \"\"\"\n",
    "    return int(client.get(DATASET_ID, query=query)[0]['count_unique_key'])\n",
    "\n",
    "get_n_records()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to download about 3 million records for 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_311_data():\n",
    "    with Socrata(COMPLAINTS_DATASET_URL, None, timeout=90) as client:\n",
    "        loop_size = 2000\n",
    "        n_loops = 1500\n",
    "        for i in range(n_loops):\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Retrieving batch {i}...\")\n",
    "            results = client.get(                                                  \n",
    "                DATASET_ID,                                                        \n",
    "                select=('unique_key,complaint_type,descriptor,borough,city,'\n",
    "                        'incident_zip,latitude,longitude'),                      \n",
    "                where='date_extract_y(created_date)=2017',                         \n",
    "                limit=loop_size,                                                  \n",
    "                offset=loop_size*i\n",
    "            )\n",
    "            pd.DataFrame.from_records(results).to_csv(f'../data/raw/nyc-311-complaints-2017-{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_population_by_zip(url):\n",
    "    \"\"\" Retrieves 2010 Census population by ZIP code data \"\"\"\n",
    "    try:\n",
    "        population_by_zip = pd.read_csv(url)\n",
    "        return population_by_zip\n",
    "    except HTTPError as e:\n",
    "        print(e)\n",
    "        \n",
    "def scrape_nyc_zips(url):\n",
    "    \"\"\" Scrapes table of NYC zipcodes from New York State Department\n",
    "    of Health website \"\"\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        return r\n",
    "    except HTTPError as e:\n",
    "        print(\"NYC neighborhood ZIP code lookup table not found:\", e)\n",
    "\n",
    "\n",
    "# TODO: Refactor to have utility functions for, for example, the\n",
    "# \"tidying\" aspects and the conversion aspects ... and rename this\n",
    "# function to something more sensible\n",
    "def tidy_nyc_zips(html):\n",
    "    \"\"\" Wrangle HTML table of NYC ZIP codes into a \"tidy\" data frame\n",
    "\n",
    "    Args:\n",
    "        html (requests.models.Response):\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: This seems too ugly and hacky so find a more elegant\n",
    "    # solution\n",
    "    borough_zips = (\n",
    "        pd.read_html(html.content, header=0)[0]\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    borough_zips.loc[borough_zips['ZIP Codes'].isnull(), 'Borough'] = np.nan\n",
    "    borough_zips.loc[:, 'ZIP Codes'] = \\\n",
    "        borough_zips.loc[:, 'ZIP Codes'].str.replace(' ', '')\n",
    "\n",
    "    borough_zips.loc[:, 'ZIP Codes'] = (\n",
    "        borough_zips.loc[:, 'ZIP Codes']\n",
    "                    .combine_first(borough_zips['Neighborhood'])\n",
    "    )\n",
    "\n",
    "    # TODO: keep the neighborhood information, even though it's not\n",
    "    # currently necessary for this analysis\n",
    "    borough_zips.drop('Neighborhood', axis=1, inplace=True)\n",
    "    borough_zips.loc[:, 'Borough'] = \\\n",
    "        borough_zips.loc[:, 'Borough'].ffill()\n",
    "\n",
    "    # Overwrite the comma-separated string \"list\" in the cell\n",
    "    # with an actual list of integers\n",
    "    borough_zips.loc[:, 'ZIP Codes'] = (\n",
    "        borough_zips.loc[:, 'ZIP Codes']\n",
    "                    .apply(lambda x: x.split(','))\n",
    "    )\n",
    "\n",
    "    # TODO: Write utility function for this pattern\n",
    "    borough_zips = (\n",
    "        borough_zips.set_index(['index', 'Borough'])\n",
    "                    .loc[:, 'ZIP Codes']\n",
    "                    .apply(pd.Series) # Expand the list of \n",
    "                    .stack()\n",
    "                    .reset_index()\n",
    "    )\n",
    "\n",
    "    borough_zips.drop(['index', 'level_2'], axis=1, inplace=True)\n",
    "    borough_zips.columns = \\\n",
    "        'borough zip_code'.split(' ')\n",
    "    borough_zips.loc[:, 'zip_code'] = \\\n",
    "        borough_zips.loc[:, 'zip_code'].astype(int)\n",
    "\n",
    "    return borough_zips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "### NYC 311 Complaints 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_311_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = !ls *.csv  # TODO: fix because this is brittle\n",
    "frames = {}\n",
    "for file_ in files:\n",
    "    frames[file_] = pd.read_csv(opj('../data/raw/', file_))\n",
    "    \n",
    "complaints = pd.concat(frames.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints.loc[:, 'unique_key'] = \\\n",
    "    complaints.loc[:, 'unique_key'].astype(int)\n",
    "\n",
    "complaints.loc[:, 'borough'] = (\n",
    "    complaints.loc[:, 'borough']\n",
    "              .str.title()\n",
    "              .astype('category')\n",
    ")\n",
    "\n",
    "complaints.loc[:, 'city'] = (\n",
    "    complaints.loc[:, 'city']\n",
    "              .str.title()\n",
    "              .astype('category')\n",
    ")\n",
    "\n",
    "complaints.loc[:, 'complaint_type'] = \\\n",
    "    complaints.loc[:, 'complaint_type'].astype('category')\n",
    "\n",
    "complaints.loc[:, 'incident_zip'] = \\\n",
    "    complaints.loc[:, 'incident_zip'].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "complaints.loc[:, 'created_date'] = \\\n",
    "    complaints.loc[:, 'created_date'].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints = complaints.replace('Unspecified', np.nan)\n",
    "\n",
    "is_borough = complaints['city'].isin(['Bronx', 'Brooklyn', 'Manhattan',\n",
    "                                      'Queens', 'Staten Island'])\n",
    "complaints.loc[~is_borough, 'city'] = np.nan\n",
    "\n",
    "complaints.loc[:, 'borough'] = (\n",
    "    complaints.loc[:, 'borough']\n",
    "              .combine_first(complaints['city'])\n",
    ")\n",
    "\n",
    "complaints.drop('city', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2010 Census Population by ZIP Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_by_zip = get_population_by_zip(POPULATION_DATASET_URL)\n",
    "population_by_zip.columns = 'zip_code population'.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYC ZIP Codes\n",
    "\n",
    "There are some non-NYC ZIP codes in the dataset, so we'd like to safely filter those out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = scrape_nyc_zips(NYC_ZIPCODES_DATASET_URL)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = scrape_nyc_zips(NYC_ZIPCODES_DATASET_URL)\n",
    "nyc_zips = tidy_nyc_zips(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_zips.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_by_zip_nyc = nyc_zips.merge(\n",
    "    population_by_zip,\n",
    "    on='zip_code',\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_by_zip_nyc.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_by_borough = (\n",
    "    population_by_zip_nyc.groupby('borough', as_index=False)\n",
    "                          .population\n",
    "                           .sum()\n",
    ")\n",
    "population_by_borough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Google, NYC had a population of 8.194 million in 2010. The scraped and merged dataset says that the 2010 population is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_by_borough['population'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It isn't exactly the same, but it's close enough for the present purposes to ignore futher investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_by_zip_nyc.to_csv('../data/cleaned/population-by-zip-nyc-2010.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints = complaints.merge(\n",
    "    population_by_zip_nyc[['borough', 'population']],\n",
    "    left_on='incident_zip',\n",
    "    right_on='zip_code',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "complaints.loc[:, 'borough'] = (\n",
    "    complaints.loc[:, 'borough_x']\n",
    "              .combine_first(complaints['borough_y'])\n",
    ")\n",
    "\n",
    "complaints.drop(['borough_x', 'borough_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order = [\n",
    "    'unique_key', 'created_date', 'borough', 'zip_code', \n",
    "    'latitude', 'longitude', 'complaint_type', 'descriptor'\n",
    "]\n",
    "                \n",
    "complaints[column_order].to_parquet(\n",
    "    'data/cleaned/nyc-311-complaints-raw.csv',\n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
